[2025-03-16 00:12:35,434] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-16 00:12:37,786] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-16 00:12:37,786] [INFO] [runner.py:607:main] cmd = /home/grinner/anaconda3/envs/comedy/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=12390 --enable_each_rank_log=None training/step1_supervised_finetuning/main.py --model_name_or_path /home/grinner/Work_space//models/llama-2-7b --train_data_path ./processed_data/train.jsonl --valid_data_path ./processed_data/valid.jsonl --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --data_output_path ./output/step1//data --max_seq_len 2048 --learning_rate 1e-5 --weight_decay 0.1 --num_train_epochs 3 --num_train_samples 90913 ./processed_data/train.jsonl --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 400 --seed 42 --zero_stage 2 --save_interval 2000 --log_interval 100 --eval_interval 1000 --output_dir ./output/step1//2025-03-16-00.12.33 --gradient_checkpointing --tensorboard_path ./output/step1/logs/
[2025-03-16 00:12:38,574] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-16 00:12:40,009] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-03-16 00:12:40,009] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-03-16 00:12:40,009] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-03-16 00:12:40,009] [INFO] [launch.py:164:main] dist_world_size=1
[2025-03-16 00:12:40,009] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-03-16 00:12:40,013] [INFO] [launch.py:256:main] process 114019 spawned with command: ['/home/grinner/anaconda3/envs/comedy/bin/python', '-u', 'training/step1_supervised_finetuning/main.py', '--local_rank=0', '--model_name_or_path', '/home/grinner/Work_space//models/llama-2-7b', '--train_data_path', './processed_data/train.jsonl', '--valid_data_path', './processed_data/valid.jsonl', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--data_output_path', './output/step1//data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--num_train_samples', '90913', './processed_data/train.jsonl', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '400', '--seed', '42', '--zero_stage', '2', '--save_interval', '2000', '--log_interval', '100', '--eval_interval', '1000', '--output_dir', './output/step1//2025-03-16-00.12.33', '--gradient_checkpointing', '--tensorboard_path', './output/step1/logs/']
[2025-03-16 00:12:42,213] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Namespace(data_path=['Dahoas/rm-static'], data_split='6,2,2', data_output_path='./output/step1//data', model_name_or_path='/home/grinner/Work_space//models/llama-2-7b', per_device_train_batch_size=4, per_device_eval_batch_size=4, max_seq_len=2048, learning_rate=1e-05, weight_decay=0.1, num_train_epochs=3, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=400, output_dir='./output/step1//2025-03-16-00.12.33', seed=42, local_rank=0, gradient_checkpointing=True, offload=False, zero_stage=2, lora_dim=0, lora_module_name='decoder.layers.', only_optimize_lora=False, tensorboard_path='./output/step1/logs/', save_interval=2000, log_interval=100, eval_interval=1000, train_data_path='./processed_data/train.jsonl', valid_data_path='./processed_data/valid.jsonl', num_train_samples=90913, ntk_RoPE_scaling_ratio=1, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None)
['./processed_data/train.jsonl']
[2025-03-16 00:12:42,975] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-16 00:12:42,976] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[rank0]:[W316 00:12:43.059739550 ProcessGroupNCCL.cpp:4693] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/grinner/Work_space/COMEDY/training/step1_supervised_finetuning/main.py", line 412, in <module>
[rank0]:     main()
[rank0]:   File "/home/grinner/Work_space/COMEDY/training/step1_supervised_finetuning/main.py", line 235, in main
[rank0]:     model = create_hf_model(AutoModelForCausalLM, args.model_name_or_path,
[rank0]:   File "/home/grinner/Work_space/COMEDY/training/utils/model/model_utils.py", line 41, in create_hf_model
[rank0]:     model = model_class.from_pretrained(
[rank0]:   File "/home/grinner/anaconda3/envs/comedy/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
[rank0]:     return model_class.from_pretrained(
[rank0]:   File "/home/grinner/anaconda3/envs/comedy/lib/python3.9/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/grinner/anaconda3/envs/comedy/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3808, in from_pretrained
[rank0]:     raise EnvironmentError(
[rank0]: OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/grinner/Work_space//models/llama-2-7b.
[rank0]:[W316 00:12:43.725634667 ProcessGroupNCCL.cpp:1497] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-16 00:12:46,020] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 114019
[2025-03-16 00:12:46,020] [ERROR] [launch.py:325:sigkill_handler] ['/home/grinner/anaconda3/envs/comedy/bin/python', '-u', 'training/step1_supervised_finetuning/main.py', '--local_rank=0', '--model_name_or_path', '/home/grinner/Work_space//models/llama-2-7b', '--train_data_path', './processed_data/train.jsonl', '--valid_data_path', './processed_data/valid.jsonl', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--data_output_path', './output/step1//data', '--max_seq_len', '2048', '--learning_rate', '1e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--num_train_samples', '90913', './processed_data/train.jsonl', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '400', '--seed', '42', '--zero_stage', '2', '--save_interval', '2000', '--log_interval', '100', '--eval_interval', '1000', '--output_dir', './output/step1//2025-03-16-00.12.33', '--gradient_checkpointing', '--tensorboard_path', './output/step1/logs/'] exits with return code = 1
